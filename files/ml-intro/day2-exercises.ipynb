{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPIA DS Workshop (June 2024)\n",
    "# Introduction to ML with Python: Day 2 -- Neural Networks with Pytorch\n",
    "\n",
    "This Notebook is part of the Workshop we gave at MPIA to introduce machine learning.\n",
    "\n",
    "**Contributors**: Morgan Fouesneau & Iva Momcheva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The world of data analysis and machine learning is growing rapidly. As a scientist, you may be wondering what deep learning and neural networks are all about. I provide a short introduction to these topics with an emphasis on their potential applications in the field of data analysis and astronomy.\n",
    "\n",
    "Deep Learning refers to artificial neural networks (ANNs) that use multiple layers of neurons for more complex tasks such as pattern recognition or image classification. Deep Learning models can learn patterns from large amounts of data by adjusting connections between different layers until desired performance is achieved - making them ideal for complex problems such as computer vision or natural language processing (NLP).\n",
    "\n",
    "Python libraries like TensorFlow, Keras, PyTorch, Theano, Jax, etc., allow scientists to easily create powerful deep learning models without having extensive knowledge on coding algorithms themselves. These libraries enable researchers and developers alike to quickly prototype new ideas using pre-existing architectures while still allowing flexibility when needed for custom implementations. These libraries may be too easy to use for research and development purposes, as often users do not need some understanding to obtain the results of their research leading to sometimes mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#f66a0a\">\n",
    "‚ö†Ô∏è This notebook requires intermediate Python knowledge, in particular object oriented programming. If you are not familiar with these concepts, you need to review them before proceeding.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup project environement\n",
    "\n",
    "First, we define a virtual environment for our project. This is a good practice to avoid conflicts between different projects.\n",
    "Details on how to create a virtual environment can be found on our FAQ [here](https://mpi-astronomy.github.io/FAQ/chapters/python/virtualenv.html)\n",
    "\n",
    "Once done, we can activate the environment and install the required packages. We will use the following packages (for both days):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file requirements.txt\n",
    "\n",
    "matplotlib\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "torch\n",
    "astropy\n",
    "shap\n",
    "tqdm\n",
    "ipywidgets\n",
    "requests\n",
    "graphviz  # only for schematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the packages using the following command (after activating your virtual environment):\n",
    "```bash\n",
    "python3 -m pip install -q -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those using Mac M-chips you should be able to use the GPU with Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Verify that we have MPS support in PyTorch\n",
    ">\n",
    "> MPS is a feature of Apple's Metal API that allows to use the M-series' GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(f\"Running on {mps_device} device(s): {x}\")\n",
    "elif torch.cuda.is_available():\n",
    "    cu_device = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=cu_device)\n",
    "    print(f\"Running on {cu_device} device(s): {x}\")\n",
    "else:\n",
    "    print (\"GPU compatible device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup notebook\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "# The following is only for the presentation, you can ignore it\n",
    "from IPython.display import Markdown\n",
    "# plt.style.use('fivethirtyeight')\n",
    "# to keep everything reproducible\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and reminders from Day 1\n",
    "\n",
    "### What is deep learning?\n",
    "In its essence, Deep Learning (DL) is one of many techniques collectively known as machine learning. Machine learning (ML) refers to techniques where a computer ‚Äúlearns‚Äù by examples. Machine learning is part of the grand scheme of automated machine decisions. People often refer machine learning as being a form of artificial intelligence (AI). AI is broad and aims at tackling problems the way humans do but to first order designates systems that learn or adapt decision rules. In computer science, a rule-based system is a system that stores and manipulate knowledge to interpret information or take decisions in a useful way. Normally, the term rule-based system only applies to systems involving human-crafted or curated rule sets. For instance in a factory, if an item is not conforming to the size or shape rule, it will be discarded.\n",
    "\n",
    "![ai_ml_dl](https://mfouesneau.github.io/astro_ds/_images/AI_ML_DL.png)\n",
    "> The place and role of AI, ML, and Deep Learning in automated machine decisions. Image adapted from Tukijaaliwa, CC BY-SA 4.0, via Wikimedia Commons, [original source](https://en.wikipedia.org/wiki/File:AI-ML-DL.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks of neural networks\n",
    "\n",
    "#### Neurons\n",
    "\n",
    "A first unit of neural networks is the **neuron**. A neuron takes a set of inputs $(x_1, \\ldots, x_n)$, does a weighted sum and apply some simple function to the latter to produce one output. The following is a schematics of a neuron with three inputs.\n",
    "\n",
    "![neuron](https://mfouesneau.github.io/astro_ds/_images/fcd191316349dd6c2d1856a6eba4fd75a4872f7c34316b43f5e53238795a0fe4.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the output corresponds to $output=f(w_1 \\cdot x_1 + w_2 \\cdot x_2 + w_3 \\cdot x_3)$. $f$ is called activation function. The activation function is used to turn an unbounded input into an output that has a predictable form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Neural Network Activation Function?\n",
    "An Activation Function is a function applied on the output of a neuron and decides whether it should be ‚Äúactivated‚Äù. It does not have to output a binary decision. The role of the activation function is to rank the input importance in the process of prediction using simple mathematical operations.\n",
    "\n",
    "The primary role of the Activation Function is to transform the summed weighted input from the node into an output value fed to the next network layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° In deep learning, the Activation Function is often referred to as a Transfer Function in Artificial Neural Network.\n",
    "\n",
    "The following figure illustrates some of the most common activation functions. These do not have to be bounded to outputs in [0, 1] intervals. Note also that activations functions could be parametric and those parameters adjusted during the training phase similarly to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "    return np.maximum(alpha*x, x)\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def swich(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def selu(x, alpha = 1.67, scale = 1.05):\n",
    "    return scale * (np.where(x > 0, x, alpha * (np.exp(x) - 1)))\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1.0 + erf(x / np.sqrt(2.0)))\n",
    "\n",
    "# Plot the activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "labels = ['linear', 'sigmoid','tanh', 'ReLU', 'leaky_ReLU', 'ELU', 'swich', 'SELU', 'GELU']\n",
    "fns = [linear, sigmoid, tanh, relu, leaky_relu, elu, swich, selu, gelu]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (fn, label) in enumerate(zip(fns, labels), 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.plot(x, fn(x), label=label, lw=2)\n",
    "    plt.text(0.04, 0.9, label,\n",
    "             transform=plt.gca().transAxes,\n",
    "             fontsize='larger',\n",
    "             va='center',\n",
    "             path_effects = [pe.Stroke(linewidth=5, foreground='w'),\n",
    "                             pe.Normal()])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.suptitle('Common activation functions', fontsize='x-large', color=\"#777777\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Example of activation function\n",
    "\n",
    "Let's assume we have a three-input neuron with a ReLU activation function and the following weights:\n",
    "\n",
    "$$ w=[0, 1, 4]. $$\n",
    "\n",
    "What will be the output to an input vector $x = [2, 3, 1]$ and $x^\\prime = [1, -3, 0]$?\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "    y &= f(w \\cdot x) \\\\\n",
    "    &= f([0, 1, 4] \\cdot [2, 3, 1]) \\\\\n",
    "    &= f(7) \\\\\n",
    "    &= 7\n",
    "\\end{aligned} $$\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "    y^\\prime &= f(w \\cdot x^\\prime) \\\\\n",
    "    &= f([0, 1, 4] \\cdot [1, -3, 0]) \\\\\n",
    "    &= f(-3) \\\\\n",
    "    &= 0\n",
    "\\end{aligned} $$\n",
    "\n",
    "üîë This process of passing inputs forward to get an output is known as **feedforward**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è What value do we obtain with a sigmoid function? What about other activation functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose the right Activation Function?\n",
    "\n",
    "Activation functions could vary within a network. But any choice need to match activation functions to the outputs based on the type of problem to solve.\n",
    "\n",
    "As a rule of thumb, _ReLU_ is a good activation function to start with in most problems. But it won't always provide the best performance. But the following list gives some guidelines:\n",
    "\n",
    "* **_ReLU_** activation function should **only be for the hidden layers**.\n",
    "* **_Sigmoid_** and **_Tanh_** functions are **not for in hidden layers** as they make the model more susceptible to training issues(vanishing gradients).\n",
    "* **_Swish_** function is recommended in neural **networks with 40 hidden layers and more**.\n",
    "\n",
    "The following provides some recommendation for the output layer based on the type of prediction problem that you are solving:\n",
    "\n",
    "* Regression: Linear Activation Functions\n",
    "* Binary Classification: Sigmoid/Tanh Activation Function\n",
    "* Multiclass Classification: Softmax\n",
    "* Multilabel Classification: Sigmoid\n",
    "\n",
    "The activation function in hidden layers is typically dependent on the type of neural network architecture. As a first try, we will use the following activation function:\n",
    "\n",
    "* Convolutional Neural Network (CNN): ReLU (or SeLU etc)\n",
    "* Recurrent Neural Network: Tanh and/or Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° All hidden layers usually use the same activation function. However, the output layer can typically use a different activation function from the hidden layers. The choice depends on the goal or type of prediction made by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with coding a Neuron\n",
    "\n",
    "Let's manually implement a neuron using NumPy.\n",
    "\n",
    "1. first define a ReLU activation function as $f(x) = \\max(0, x)$\n",
    "2. define a class `Neuron` which takes a vector of weights at initialization\n",
    "3. define the `Neuron.forward` method which takes a vector of inputs and returns the output of the neuron\n",
    "4. evaluate the code with the previous values.\n",
    "\n",
    "```python\n",
    "class Neuron:\n",
    "  def __init__(self, weights: np.ndarray, activation: Callable):\n",
    "    pass\n",
    "\n",
    "  def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è Code the neuron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "# activation functions defined above\n",
    "\n",
    "class Neuron:\n",
    "    # code here\n",
    "    ...\n",
    "\n",
    "\n",
    "weights = np.array([0, 1, 4])\n",
    "n = Neuron(weights, relu)\n",
    "print(\"[2, 3, 1] --> \", n.forward(np.array([2, 3, 1])))\n",
    "print(\"[1, -3, 0] --> \", n.forward(np.array([1, -3, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Neurons to create a network\n",
    "\n",
    "With a single neuron, we can create a scaled weighted sum operator. With a linear activation function, a single neuron corresponds to a linear regression model.\n",
    "\n",
    "But with many connected neurons we can generate more complex operations.\n",
    "\n",
    "The following example network has 3 inputs, a single hidden layer with 3 neurons an one output layer with 1 neuron.\n",
    "\n",
    "A **hidden layer** corresponds to any layer between the input (first) layer and output (last) layer. There can be many hidden layers (deep network)\n",
    "\n",
    "![network](https://mfouesneau.github.io/astro_ds/_images/440ab20f7db6b184562fcc207637bc1381eebedb56a90b25942cf2a59551881f.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network can have any number of layers with any number of neurons in those layers. The basic idea stays the same: feed the input(s) forward through the neurons in the network to get the output(s) at the end. For simplicity, we‚Äôll keep using the network pictured above for the rest of this post.\n",
    "\n",
    "We will skip the pen and paper calculations and implement this network in Python.\n",
    "\n",
    "```python \n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "        input_size: int, \n",
    "        hidden_size: int, \n",
    "        output_size: int, \n",
    "        weights: List[float] = None, \n",
    "        activation: Callable = sigmoid):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "``` \n",
    "\n",
    "```python\n",
    "model = NeuralNetwork(3, 3, 1, activation=sigmoid)\n",
    "model.forward([2, 3, 1])\n",
    "```\n",
    "\n",
    "> üí° This implementation is freeform. You can use the previous `Neuron` class, make a `Dense` layer or work with arrays of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è Code the NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementatation generalizes the number of hidden layers\n",
    "# reuses the previous activation functions\n",
    "# However we do not use the Neuron class here\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,\n",
    "        input_size: int,\n",
    "        hidden_size: List[int],\n",
    "        output_size: int,\n",
    "        weights: List[float] = None,\n",
    "        activation: Callable = relu):\n",
    "        # your code here\n",
    "        ...\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Perform forward propagation \"\"\"\n",
    "        # your code here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the NeuralNetwork class\n",
    "model = NeuralNetwork(3, [3], 1, activation=sigmoid)\n",
    "model.forward([2, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are initialized at random and therefore without imposing the weights and doing the calculations by hand, it will be difficult to verify the model. Let's train our model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Neural Network\n",
    "\n",
    "Let's use the common `iris` dataset, a multivariate data set used and made famous by the British statistician and biologist Ronald Fisher in his 1936 paper.\n",
    "\n",
    "The data set consists of $50$ samples from each of three species of Iris (setosa, virginica, and versicolor) and four measurements from each sample: the length and the width of the sepals and petals, in centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dataset, let's create a neural network model to predict the species given the measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è define the model variable and call `.forward()` on the model with the first row of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "model = ...  # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Loss function, objective function\n",
    "\n",
    "Before we train our network, we first need to define a performance metric which the training procedure will optimize.\n",
    "\n",
    "For simplicity here, we will use the Mean Squared Error (MSE) as the loss function:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2, $$\n",
    "\n",
    "where $N$ is the number of samples, $y_i$ is the true label, and $\\hat{y}_i$ the output prediction of the network.\n",
    "The better our predictions, the lower MSE value, i.e. lower loss.\n",
    "\n",
    "> üí° The training procedure minimizes the loss function given a training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true: np.array, y_pred: np.array) -> float:\n",
    "    # your code here\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.array([model.forward(xk) for xk in X])\n",
    "print(\"MSE loss (t=0) = \", mse(y, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to optimize this function, i.e. the loss of the neural network. We can adjust the network‚Äôs weights to change the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization and backpropagation\n",
    "\n",
    "> ‚ö†Ô∏è This part is a bit mathematical. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function $L$ is function of all the weights $W$ and we want to minimize $L$ w.r.t $W$.\n",
    "\n",
    "Let's look at optimizing a single weight $w_{11}$ corresponding to the connection of the $x_{1}$ with the first neuron in the hidden layer $h_1$. The derivative of $L$ w.r.t. $w_{11}$ is \n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_{11}} &= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w_{11}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} &= -2 (y - \\hat{y})\n",
    "\\end{align}$$\n",
    "\n",
    "The other term is a bit more complicated. But since $w_{11}$ is between $x_1$ and $h_1$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\hat{y}}{\\partial w_{11}} &= \\frac{\\partial \\hat{y}}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_{11}} \\\\\n",
    "\\frac{\\partial \\hat{y}}{\\partial h_1} &= w_{21} \\cdot f^\\prime(w_{21} h_1 + w_{22} h_2 + w_{23} h_3)\\\\\n",
    "\\frac{\\partial h_1}{\\partial w_{11}} &= x_1 \\cdot f^\\prime(w_{11} x_1 + w_{12} x_2 + w_{13} x_3)\n",
    "\\end{align}$$\n",
    "\n",
    "Finally, most activation functions $f$ have relatively simple derivative $f^\\prime$. For instance, the sigmoid function has a simple derivative \n",
    "\n",
    "$$\\begin{align}\n",
    "f(x) &= \\frac{1}{1 + e^{-x}} \\\\\n",
    "f^\\prime(x) &= \\frac{e^{-x}}{(1 + e^{-x})^2} = f(x) \\cdot (1 - f(x))\n",
    "\\end{align}$$\n",
    "\n",
    "We managed to express all the necessary terms in the derivative of $L$ w.r.t. $w_{11}$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_{11}} &= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h_{1}} \\cdot \\frac{\\partial {h_1}}{\\partial w_{11}}\n",
    "\\end{align}$$\n",
    "\n",
    "This system of calculating partial derivatives by working backwards is known as **backpropagation**. Backpropagation is the process of finding the derivative of the loss function w.r.t. all the weights $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training through Stochastic Gradient Descent\n",
    "\n",
    "The final step of our model is to minimize the loss function w.r.t. all the weights using our training data.\n",
    "\n",
    "A traditional approach uses an optimization algorithm essentially based on stochastic gradient descent (SGD).\n",
    "It is a variant of Gradient Descent (GD) but it uses only a random subset of the training data (also known as a mini-batch) rather than the entire training data set to calculate the gradient of the loss function. This random selection of data provides faster and more frequent updates to the model parameters, making it more efficient than GD when the data set is large. SGD repeats this process for a fixed number of iterations or until the convergence of the loss function. \n",
    "\n",
    "The learning rate $\\eta$ is an hyperparameter that controls the step size at each iteration, e.g.:\n",
    "\n",
    "$$ w_1 \\leftarrow w_{11} - \\eta \\frac{\\partial L}{\\partial w_{11}} $$\n",
    "\n",
    "If we do this for every weight in the network, the loss will slowly decrease and our network will improve.\n",
    "\n",
    "The SGD training process will look like this:\n",
    "1. Choose one random sample/subset from our dataset. \n",
    "2. Calculate all the partial derivatives of loss with respect to weights\n",
    "3. Update the weights by subtracting the learning rate times the partial derivatives.\n",
    "and repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to code everything in the following we use `sklearn.neural_network.MLPClassifier` which implements an SGD optimizer (as well as others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è call the MLPClassifier with the iris dataset and fit the model. Use the `solver='sgd'` and set the learning rate `alpha`, `hidden_layer_sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = ...\n",
    "\n",
    "model.fit(X, y)\n",
    "yhat = model.predict(X)\n",
    "print(\"Final loss: \", mse(y, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the loss values as function of the training iterations, aka **epoch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.loss_curve_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet')\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.title('training data')\n",
    "plt.subplot(1, 2, 2)\n",
    "yhat = model.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=yhat, cmap='jet')\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.title('predictions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è The above example is also an example of bad practice. We should use a validation set to monitor the performance of the model and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are deep neural networks hard to train?\n",
    "\n",
    "Training NNs may sometimes lead to catastrophic failures (e.g., wrong activation function, forgetting to normalize the input data). There are two major challenges you might encounter when training your deep networks.\n",
    "\n",
    "#### Vanishing Gradients\n",
    "\n",
    "Like the sigmoid function, certain activation functions compress an infinite interval into [0, 1]. Hence, their derivative becomes small. For shallow networks with only a few layers that use these activations, this isn‚Äôt a big problem. But, when networks contain more layers, the product of those gradients becomes too small for training to work effectively.\n",
    "\n",
    "#### Exploding Gradients\n",
    "\n",
    "Exploding gradients occurs when significant error gradients accumulate and result in very large updates to neural network model weights during training with SGD based optimizers. With these exploding gradients, you can obtain unstable networks and the learning cannot be completed.\n",
    "\n",
    "The values of the weights can also become so large as to overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Key vocabulary\n",
    "\n",
    "* **Artificial Neural Network** (ANN): A computational system inspired by the way biological neural networks in the human brain process information.\n",
    "* **Neuron**: A building block of ANN. It is responsible for accepting input data, performing calculations, and producing output.\n",
    "* **Deep Neural Network**: An ANN with many layers placed between the input layer and the output layer.\n",
    "* **Input data**: Information or data provided to the neurons.\n",
    "* **Weights**: The strength of the connection between two neurons. Weights determine what impact the input will have on the output.\n",
    "* **Bias**: An additional parameter used along with the sum of the product of weights and inputs to produce an output.\n",
    "* **Activation Function**: Determines the output of a neural network.\n",
    "* **Feed forward** (forward propagation): The process of feeding the input data to the neurons.\n",
    "* **Back propagation** (back propagation): The process of adjusting the weights and biases of the neurons through the gradient of the loss function.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages & Disadvantages of Neural Networks\n",
    "\n",
    "#### Advantages \n",
    "* Fault tolerance: even if a few neurons are not \"working\" properly, that would not prevent the neural networks from generating outputs.\n",
    "\n",
    "* Real-time Operations: neural networks can learn synchronously and easily adapt to their changing environments.\n",
    "\n",
    "* Adaptive Learning: neural networks can learn how to work on different tasks, or sub-tasks with proper training.\n",
    "\n",
    "* Parallel processing capacity: neural networks can perform multiple tasks simultaneously.\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "* Unexplained behavior of the network: neural networks provide a solution to a problem but rarely a why and how it made the decisions it made. Understanding the behavior of a model represents additional tasks to the architect (i.e., the user).\n",
    "\n",
    "* Determination of appropriate network structure: there is no generic rule (or rule of thumb) to design a neural network architecture. A proper network structure is achieved by trying the best network, in a trial and error approach. It is a process that involves refinement.\n",
    "\n",
    "* Hardware dependence: To be efficient in computational speed (training or evaluation), neural networks require (or are highly dependent on) processors (CPU, GPU, TPU) with adequate processing capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What sort of problems can Deep Learning not solve?\n",
    "\n",
    "DL is not always capable to solve problems. It will not work in\n",
    "* any case where only a small amount of training data is available,\n",
    "* tasks requiring an explanation of how the answer was arrived at (interpretability not guaranteed),\n",
    "* Classifying things which are nothing like their training data (not good at genaralization).\n",
    "\n",
    "### What sort of problems can Deep Learning solve but should not be used for?\n",
    "\n",
    "Deep Learning needs a lot of computational power, it often relies on specialised hardware like graphical processing units (GPUs). Many computational problems can be solved using less intensive techniques, but could still technically be solved with Deep Learning.\n",
    "It is tempting to use the fashion techniques to solve old problems, but we should remain critical.\n",
    "\n",
    "Deep learning can technically solve the following but it would probably be a wasteful way to do it:\n",
    "\n",
    "* Logic operations, such as sums, averages, ranges etc. \n",
    "* Modelling well defined systems, where the equations governing them are known and understood.\n",
    "* Basic computer vision tasks such as edge detection, decreasing colour depth or blurring an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Libraries in Python\n",
    "\n",
    "There are many software libraries available for Deep Learning. Python includes:\n",
    "\n",
    "### PyTorch\n",
    "[PyTorch](https://pytorch.org/) was developed by Facebook in 2016 and it is a popular choice for Deep Learning applications. It was developed for Python from the start and feels a lot more ‚Äúpythonic‚Äù than TensorFlow. Like TensorFlow it was designed to do more than just Deep Learning and offers some very low level interfaces. PyTorch Lightning offers a higher level interface to PyTorch to set up experiments. Like TensorFlow it is also very easy to integrate PyTorch with a GPU. \n",
    "\n",
    "### Jax + Flax\n",
    "\n",
    "[Jax](https://jax.readthedocs.io/) is a Python library developed by Google for high-performance numerical computing and machine learning. It provides an interface that is similar to NumPy but that can run on GPUs and TPUs transparently for the users offering significantly improved performance. JAX includes automatic differentiation, which makes it easy to train machine learning models using techniques like stochastic gradient descent. \n",
    "\n",
    "[Equinox](https://docs.kidger.site/equinox/) is a neural network library that is built on top of JAX (alternative to flax) with easy-to-use PyTorch-like syntax. It provides at most wrappers around JAX, which makes it more flexible than other libraries.\n",
    "\n",
    "[Flax](https://flax.readthedocs.io/) is a neural network library that is built on top of JAX and is designed for flexibility. It provides a high-level API for defining and training neural networks and has built-in support for advanced training techniques like mixed precision and gradient checkpointing. \n",
    "\n",
    "### Keras\n",
    "[Keras](https://keras.io/) is designed to be easy to use and usually requires fewer lines of code than other libraries. We have chosen it for this workshop for that reason. Keras can actually work on top of TensorFlow (and several other libraries), hiding away the complexities of TensorFlow while still allowing you to make use of their features.\n",
    "\n",
    "The performance of Keras is sometimes not as good as other libraries and if you are going to move on to create very large networks using very large datasets then you might want to consider one of the other libraries. But for many applications the performance difference will not be enough to worry about and the time you will save with simpler code will exceed what you will save by having the code run a little faster.\n",
    "\n",
    "### TensorFlow\n",
    "[TensorFlow](https://www.tensorflow.org/) was developed by Google and is one of the older Deep Learning libraries, ported across many languages since it was first released to the public in 2015. It is very versatile and capable of much more than Deep Learning but as a result it often takes a lot more lines of code to write Deep Learning operations in TensorFlow than in other libraries. It offers (almost) seamless integration with GPU accelerators and Google‚Äôs own TPU (Tensor Processing Unit) chips that are built specially for machine learning.\n",
    "\n",
    "> ‚ö†Ô∏è Tensorflow is about to be deprecated by Google (not further dev.) in favor of JAX.\n",
    "\n",
    "### Which library should I use/learn?\n",
    "\n",
    "There are no standard answer to which library should I use or learn, even when having a specific problem.\n",
    "\n",
    "TensorFlow and PyTorch are currently the most widely used deep learning libraries, while JAX is growing in popularity due to its efficient GPU and TPU acceleration, automatic differentiation, and high flexibility. \n",
    "\n",
    "It is important to note that Google currently slows down the development of Tensorflow in favor of JAX. But Tensorflow remains important for insdustry applications. It provides many services and pre-trained models.\n",
    "\n",
    "Jax is the new kid in the playground. Its development is extremely fast, and it gets a lot of traction in the scientific community. (A few developpers are former-astronomers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Logical XOR network\n",
    "\n",
    "In this exercise, you will implement a simple XOR network, i.e. implement a network that exactly matches the behavior of a simple logical XOR operator.\n",
    "\n",
    "For this exercise, the network needs at least one hidden layer to solve the problem.\n",
    "\n",
    "### Logical XOR \n",
    "\n",
    "XOR stands for \"exclusive or\". The output of the XOR function has only a true value if the two inputs are different. If the two inputs are identical, the XOR function returns a false value. The following table shows the inputs and outputs of the XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, x2, y\n",
    "xor_data = np.array([[0, 0, 0],\n",
    "                     [1, 0, 1],\n",
    "                     [0, 1, 1],\n",
    "                     [1, 1, 0]])\n",
    "or_data = np.array([[0, 0, 0],\n",
    "                    [1, 0, 1],\n",
    "                    [0, 1, 1],\n",
    "                    [1, 1, 1]])\n",
    "and_data = np.array([[0, 0, 0],\n",
    "                     [1, 0, 0],\n",
    "                     [0, 1, 0],\n",
    "                     [1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure illustrates the logical operations OR, AND, and XOR. \n",
    "In contrast to OR and AND problems, the XOR one cannot be linearly separated by a single boundaryline. To solve this problem, we will need two boundary lines, hence requires at least one hidden layer in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(xor_data, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.plot([0, 1, 1, 0, 0], [0, 0, 1, 1, 0], color='k')\n",
    "    ax.scatter(xor_data[:, 0], xor_data[:, 1], c=xor_data[:, 2],\n",
    "               cmap='Paired_r', s=200, zorder=10, edgecolors='w')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    plt.setp(ax.spines.values(), color='w')\n",
    "    ax.set_xticks([0, 1], [0, 1])\n",
    "    ax.set_yticks([0, 1], [0, 1])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "ax = plt.subplot(131, aspect='equal')\n",
    "plot_data(or_data, ax)\n",
    "plt.plot([-0.05, 0.8], [0.5, -0.04], color='C3', linestyle='-', lw=5)\n",
    "plt.text(0.5, 0.5, \"OR\", ha='center', va='center', transform=ax.transAxes, fontsize='x-large')\n",
    "ax = plt.subplot(132, aspect='equal')\n",
    "plot_data(and_data, ax)\n",
    "plt.plot([0.5, 1.04], [1.05, 0.4], color='C3', linestyle='-', lw=5)\n",
    "plt.text(0.5, 0.5, \"AND\", ha='center', va='center', transform=ax.transAxes, fontsize='x-large')\n",
    "ax = plt.subplot(133, aspect='equal')\n",
    "plot_data(xor_data, ax)\n",
    "plt.plot([-0.05, 0.8], [0.5, -0.04], color='C3', linestyle='-', lw=5)\n",
    "plt.plot([0.5, 1.04], [1.05, 0.4], color='C3', linestyle='-', lw=5)\n",
    "plt.text(0.5, 0.5, \"XOR\", ha='center', va='center', transform=ax.transAxes, fontsize='x-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define a Neural Network that is able to solve the XOR problem. \n",
    "With the correct choice of functions and weight parameters, a Neural Network with one hidden layer is able to solve the XOR problem.\n",
    "Because we have logical operations, a threshold activation function should be sufficient.\n",
    "\n",
    "![xor](https://mfouesneau.github.io/astro_ds/_images/6f4674816e1e53d870846fd43586f325dbc19d93506e6253302a52f631a6dc88.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use \n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "model = MLPClassifier(\n",
    "                activation='logistic',\n",
    "                max_iter=20_000,\n",
    "                hidden_layer_sizes=(2,),\n",
    "                solver='lbfgs',\n",
    "                )\n",
    "\n",
    "X = xor_data[:, :2]\n",
    "y = xor_data[:, 2]\n",
    "model.fit(X, y)\n",
    "\n",
    "X_, Y_ = np.meshgrid(np.linspace(0, 1, 200), np.linspace(0, 1, 200))\n",
    "Z = model.predict(np.c_[X_.ravel(), Y_.ravel()])\n",
    "\n",
    "ax = plt.subplot(111, aspect='equal')\n",
    "plot_data(xor_data, ax)\n",
    "Z = Z.reshape(X_.shape)\n",
    "plt.pcolormesh(X_, Y_, Z, cmap=plt.cm.Paired_r)\n",
    "plt.xlim(-0.04, 1.04)\n",
    "plt.ylim(-0.04, 1.04);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear regression with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore coding neural networks to do a non-linear regression with a multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate mock data\n",
    "\n",
    "In this exercise, we generate 100 pairs of values from a non-linear function \n",
    "\n",
    "$$f(x) = x^3 - x^2 + 25 \\cdot \\sin(2x),$$\n",
    "\n",
    "and we add noise to a random uniform sample:\n",
    "\n",
    "$$ y = \\mathcal{N}(f(x), 10)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è Implement the function `f(x)` and generate a 100 data points (x, y) with noise. Plot your data to make sure it looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP in PyTorch\n",
    "\n",
    "In this section, we code a simple network using PyTorch `torch.nn` library.\n",
    "\n",
    "To define a model, PyTorch requires a class definition deriving `nn.Module`. This class pre-defines neural networks properties and important methods.\n",
    "\n",
    "Below, we define our neural network class. The main aspects are\n",
    "\n",
    "1.  the **initialization**/**constructor** function `__init__`, which defines the properties of our model and parameters\n",
    "2.  the **`forward`** method which takes the model's input and computes the corresponding prediction given the current model. \n",
    "\n",
    "PyTorch allows flexibility in how to define neural network modules, we show below a concise implementation. Regardless, any implementation should work the same way eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class  MLP(nn.Module):\n",
    "    def __init__(self, layout: Sequence[int]):\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes per layer (input, ..., output)\n",
    "layout = [1, 10, 10, 10, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, our network need to take `x`, apply a series of `Linear` layer followed by an activation function until the last output layer. This corresponds to a feed foward function looking something like\n",
    "\n",
    "```python\n",
    "z = nn.Linear(1, 10)(x)\n",
    "z = nn.Linear(10, 10)(z)\n",
    "z = nn.ReLU()(z)\n",
    "z = nn.Linear(10, 10)(z)\n",
    "z = nn.ReLU()(z)\n",
    "z = nn.Linear(10, 1)(z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together, a common implementation would be the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è Implement an MLP module using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    ''' Multi-layer perceptron for non-linear regression. '''\n",
    "    def __init__(self, layout: Sequence[int]):\n",
    "        super().__init__()  # initialize following nn.Module\n",
    "        ...\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "model = MLP(layout)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the current parameter values of this model instance like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(model.parameters())\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "1. What are these parameters? (Hint: they are in oder of the sequence and computations in the forward method.)\n",
    "\n",
    "2. Guess how the weights are initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our training data, `Dataset`\n",
    "\n",
    "In PyTorch, there are multiple ways to prepare a dataset for the model training. \n",
    "The main aspect is to convert the data into tensors which are transparently moved to GPUs if needed.\n",
    "\n",
    "To make a standard API that works with small and large datasets, PyTorch provides two data primitives: \n",
    "* `torch.utils.data.Dataset` to store the data in a reusable format,\n",
    "* `torch.utils.data.DataLoader` takes a `Dataset` object as input and returns an iterable to enable easy access to the training data (especially useful for batch training).\n",
    "\n",
    "#### `Dataset`\n",
    "\n",
    "To define a `Dataset` object, we have to write a class deriving the latter and specify two key methods:\n",
    "\n",
    "1.  `__len__`, which tells subsequent applications how many data points there are in total (useful if you load the data by chunk); and\n",
    "2.  the `__getitem__` function, which takes an index as input and outputs a corresponding (input, output) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    '''\n",
    "    Custom 'Dataset' object for our regression data.\n",
    "    Must implement these functions: __init__, __len__, and __getitem__.\n",
    "    '''\n",
    "    def __init__(self, X: np.array, Y: np.array, device=None):\n",
    "        self.X = torch.from_numpy(X.astype('float32'))\n",
    "        # self.X = torch.reshape(torch.from_numpy(X.astype('float32')), (len(X), 1))\n",
    "        self.Y = torch.from_numpy(Y.astype('float32'))\n",
    "        # self.Y = torch.reshape(torch.from_numpy(Y.astype('float32')), (len(Y), 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.X))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return(self.X[idx], self.Y[idx])\n",
    "\n",
    "# instantiate Dataset object for current training data\n",
    "d = Data(x, y)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DataLoader`\n",
    "\n",
    "Let's instanciate also the data loader (or data feeder). For this example, we will provide the data in batches of 25 random examples (out of 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DataLoader\n",
    "traindata = DataLoader(d, batch_size=25 , shuffle=True)\n",
    "traindata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`traindata` is an iterator:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(traindata, 0):\n",
    "    x_, y_ = data\n",
    "    print(i, \"x: \", x_, \"\\n  y:\", y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° The training set in our current example is small and training by batch is not necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "In many libraries, writing the training code is the most important part and varies from library to library.\n",
    "They also have common bricks:\n",
    "* the _loss function_: let's take MSE here.\n",
    "* the _optimizer_: let's take _Adam_ (a flavor of stochastic gradient descent) with a learning rate $10^{-4}$\n",
    "* the _training loop_: which for many epoch, feed the data to the model, calculate the loss function, the gradient of it (backpropagation), and update the optimizer for the next iteration. \n",
    "\n",
    "In PyTorch, this looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "layout = [1, 10, 10, 10, 1]\n",
    "model = MLP(layout)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 100_000\n",
    "\n",
    "logs = []  # keep track\n",
    "\n",
    "# the following loop could be\n",
    "# for epoch in range(0, epochs), but we'll use tqdm to provide a progress indicator\n",
    "with tqdm(range(0, epochs), total=epochs) as pbar:\n",
    "    for epoch in pbar:\n",
    "        current_loss = 0.0\n",
    "        # Iterate over the batches from the DataLoader\n",
    "        for i, batch in enumerate(traindata):\n",
    "            # Get inputs\n",
    "            x_, y_ = batch\n",
    "            # reset/Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Perform forward pass\n",
    "            # ypred = model(x_).squeeze()\n",
    "            x_2d = torch.reshape(x_, (len(x_), 1))\n",
    "            ypred = model(x_2d).squeeze()\n",
    "            # Compute loss\n",
    "            loss = loss_function(ypred, y_)\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "        # display some progress ;)\n",
    "        #if (epoch + 1) % 2500 == 0:\n",
    "        #    print(f'Loss after epoch {epoch+1:5d}: {current_loss:.4g}')\n",
    "        pbar.set_postfix({\"loss\": float(current_loss)})\n",
    "        logs.append(current_loss)\n",
    "        current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(logs)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check our predictions\n",
    "x_ = torch.from_numpy(x.astype('float32'))\n",
    "ypred = model(torch.reshape(x_, (len(x_), 1))).squeeze().cpu().detach().numpy()\n",
    "\n",
    "plt.plot(xtrue, ytrue, color='k')\n",
    "plt.plot(x, y, 'o', color='C0')\n",
    "plt.plot(x, ypred, 'o', color='C1')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Optimizers and learning curves\n",
    "\n",
    "Because Adam is very popular and it is a good choice for training neural networks, we focus on building understanding of tuning its parameters: learning rate, exponential decay rate for the momentum term, and learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(10_000)\n",
    "\n",
    "eta = 5e-4\n",
    "plt.plot(epochs, np.exp(- epochs * eta), label='good', color='C2')\n",
    "plt.text(8000, 0.05, 'good rate', color='C2')\n",
    "eta = 1e-4\n",
    "plt.plot(epochs, np.clip(np.exp(- epochs * eta), 1e-1, 10), label='low', color='C0')\n",
    "plt.text(4000, 0.7, 'low rate', color='C0')\n",
    "eta = 5e-3\n",
    "plt.plot(epochs, 1.5 - 1 / (1 + np.exp(- epochs * eta)), label='high', color='C1')\n",
    "plt.text(3000, 0.52, 'high rate', color='C1')\n",
    "plt.plot(epochs, (1.5 - 1 / (1 + np.exp(- epochs * eta))) + (eta * epochs / 10) ** 2, label='high', color='C3')\n",
    "plt.text(1800, 1.1, 'too high', color='C3')\n",
    "plt.ylim(0, 1.4)\n",
    "ax = plt.gca()\n",
    "plt.setp(ax.get_xticklabels() + ax.get_yticklabels() + ax.get_xticklines() + ax.get_yticklines(),\n",
    "         visible=False)\n",
    "plt.setp([ax.spines['top'], ax.spines['right']], visible=False)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Effects of different learning rates on Loss values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small learning rate corresponds to updates and therefore a slower model training. i.e. it would require many updates to the parameters to reach the point of minima. On the other hand, a large learning rate would mean large steps to the parameters thus often tends to divergence instead of convergence.\n",
    "\n",
    "An optimal learning rate value (default value $0.001$ for Adam) means that the optimizer would update the parameters just right to reach the local minima in a given number of epochs. Varying learning rate between $0.0001$ and $0.01$ is the typical range in most cases, but this depends on the actual optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### momentum decay $(\\beta_1, \\beta_2)$\n",
    " \n",
    "$\\beta_1$ is the exponential decay rate for the momentum term. It‚Äôs default value in PyTorch is $0.9$ (from the original paper). \n",
    "\n",
    "$\\beta_2$ is the exponential decay rate for velocity term. The default value is 0.999 in the PyTorch implementation. This value should be set as close to 1.0 as possible to help with sparse gradient.\n",
    "\n",
    "### Learning Rate Decay\n",
    " \n",
    "Learning rate decay decreases as the parameter values reach closer to the global optimal solution. This avoids overshooting the minima often resulting in faster convergence of the loss function. The PyTorch implementation refers to this as `weight_decay` with default value being zero.\n",
    "\n",
    "### Epsilon\n",
    " \n",
    "Although not a tuning hyperparameter, it is a very tiny number to avoid any division by zero error in the implementation. The default value is $10^{-8}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Explore the model's behavior\n",
    "\n",
    "1. Make sure you understand the various lines of code.\n",
    "2. Change the model layout, activations, optimizer, etc.\n",
    "3. Change the mini-batch configuration. e.g. All data at once\n",
    "\n",
    "Run the model when you change something and discuss notable differences. Explain what your observations. (If you do not see anything, that could happen. Explain why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the XOR problem with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "X = torch.Tensor([[0., 0.],\n",
    "                  [0., 1.],\n",
    "                  [1., 0.],\n",
    "                  [1., 1.]])\n",
    "\n",
    "y = torch.Tensor([0., 1., 1., 0.]).reshape(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è  train an XOR network with PyTorch: make the XOR class and train it on xor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR(nn.Module):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "    def forward(self, input):\n",
    "        ...\n",
    "\n",
    "# instantiate the model\n",
    "xor_network = XOR()\n",
    "\n",
    "# define the loss function\n",
    "\n",
    "# define the optimizer\n",
    "\n",
    "# number of epochs (e.g. 1000)\n",
    "\n",
    "# keep track of the loss values\n",
    "logs = []\n",
    "\n",
    "# train the model: training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(logs)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, Y_ = np.meshgrid(np.linspace(0, 1, 200), np.linspace(0, 1, 200))\n",
    "x1 = torch.from_numpy(np.c_[X_.ravel().astype('float32'), Y_.ravel().astype('float32')])\n",
    "Z = np.round(xor_network(x1).squeeze().cpu().detach().numpy())\n",
    "\n",
    "xor_data = np.array([[0, 0, 0],\n",
    "                     [1, 0, 1],\n",
    "                     [0, 1, 1],\n",
    "                     [1, 1, 0]])\n",
    "\n",
    "ax = plt.subplot(111, aspect='equal')\n",
    "plot_data(xor_data, ax)\n",
    "Z = Z.reshape(X_.shape)\n",
    "plt.pcolormesh(X_, Y_, Z, cmap=plt.cm.Paired_r)\n",
    "plt.xlim(-0.04, 1.04)\n",
    "plt.ylim(-0.04, 1.04);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the Photometric Redshift problem with pytorch.\n",
    "\n",
    "We revisit the problem we had in Day 1 of photometric redshift estimation using a neural network. We will use the same data as in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder, Predicting galaxy redshift via regression on 3D-HST photometry\n",
    "\n",
    "We will explore a typical machine learning problem: predicting galaxy redshifts from photometry. We will use the 3D-HST survey photometry to predict the redshift of galaxies. \n",
    "\n",
    "The 3D-HST survey is a Hubble Space Telescope Treasury program that has used 248 orbits of HST time to image four fields in the sky with grism spectroscopy (AEGIS, COSMOS, GOODS-S, UKIDSS-UDS). See [Brammer et al., 2012](http://adsabs.harvard.edu/abs/2012ApJ...758L..17B) and the [3D-HST website](https://3dhst.research.yale.edu/Home.html) for more information.\n",
    "\n",
    "The data we will use is a subset of the 3D-HST photometry and redshifts. We will use the photometry in the F125W, F140W, F160W, F606W, F814W filters, and the redshifts from the 3D-HST catalog from [Skelton et al (2014)](https://dx.doi.org/10.1088/0067-0049/214/2/24).\n",
    "\n",
    "**Reminder Redshift 101**\n",
    "\n",
    "The redshift of a galaxy is a measure of how much the wavelength of light from the galaxy has been stretched by the expansion of the Universe. The redshift $z$ is defined as \n",
    "$$z = (\\lambda_{obs} - \\lambda_{em}) / \\lambda_{em},$$\n",
    "where $\\lambda_{obs}$ is the observed wavelength of light from the galaxy and $\\lambda_{em}$ is the wavelength of the light emitted by the galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import requests\n",
    "from astropy.io import votable\n",
    "from astropy.io import fits\n",
    "\n",
    "\n",
    "def get_svo_filter_profile(identifier: str) -> np.array:\n",
    "    QUERY_URL = \"http://svo2.cab.inta-csic.es/theory/fps/fps.php\"\n",
    "    query = {\"ID\": identifier}\n",
    "    response = requests.get(QUERY_URL, params=query)\n",
    "    response.raise_for_status()\n",
    "    table = votable.parse_single_table(BytesIO(response.content))\n",
    "    tab = table.to_table()\n",
    "    return (tab[\"Wavelength\"].to(\"angstrom\").value, tab[\"Transmission\"].data)\n",
    "\n",
    "\n",
    "template_url = \"https://archive.stsci.edu/hlsps/reference-atlases/cdbs/grid/comp_qso/optical_nir_qso_sed_001.fits\"\n",
    "with fits.open(template_url) as hdu:\n",
    "    wave = hdu[1].data[\"WAVELENGTH\"]\n",
    "    flux = hdu[1].data[\"FLUX\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for z in (0, 0.8, 2, 4):\n",
    "    line_color = plt.semilogy(wave * (1 + z), flux, lw=1)[0].get_color()\n",
    "    plt.text(\n",
    "        1_200 * (1 + z),\n",
    "        5e-13,\n",
    "        f\"z={z}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        color=line_color,\n",
    "        fontsize=\"small\",\n",
    "    )\n",
    "\n",
    "plt.ylim(7e-15, 5e-13)\n",
    "plt.xlim(86, 17_500)\n",
    "plt.xlabel(\"wavelength [$\\AA$]\")\n",
    "plt.ylabel(\"flux [erg/s/cm$^2$/$\\AA$]\")\n",
    "\n",
    "ax = plt.twinx()\n",
    "which_filters = [\n",
    "    \"HST/ACS_HRC.F606W\",\n",
    "    \"HST/ACS_HRC.F814W\",\n",
    "    \"HST/WFC3_IR.F125W\",\n",
    "    \"HST/WFC3_IR.F140W\",\n",
    "    \"HST/WFC3_IR.F160W\",\n",
    "]\n",
    "for fname in which_filters:\n",
    "    wave, trans = get_svo_filter_profile(fname)\n",
    "    ax.plot(wave, trans, color=\"black\", lw=1, label=fname)\n",
    "    ax.fill_between(wave, trans, color=\"k\", alpha=0.05)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(False)\n",
    "ax.set_ylabel(\"Transmission\")\n",
    "\n",
    "Markdown(\"\"\"\n",
    "> **Figure**: Quasar template from [Glikman et al. 2006](https://ui.adsabs.harvard.edu/abs/2006ApJ...640..579G/abstract), and passbands from [The SVO Filter Profile Service](http://svo2.cab.inta-csic.es/theory/fps/) ([Rodrigo, C., Solano, E., 2020](https://ui.adsabs.harvard.edu/abs/2020sea..confE.182R/abstract))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.utils.data import download_file\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "if not (os.path.exists(\"3dhst_master.phot.v4.1.cat\")):\n",
    "    file_url = \"https://archive.stsci.edu/missions/hlsp/3d-hst/RELEASE_V4.0/Photometry/3dhst_master.phot.v4.1.tar\"\n",
    "    tarfile.open(download_file(file_url, cache=True), \"r:\").extract(\n",
    "        \"3dhst_master.phot.v4.1/3dhst_master.phot.v4.1.cat\", \".\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fname = '3dhst_master.phot.v4.1/3dhst_master.phot.v4.1.cat'\n",
    "with open(fname, 'r') as fin:\n",
    "    header = fin.readline()[1:]  #remove comment character\n",
    "    df = pd.read_csv(fin, names=header.strip().split(), comment='#', delimiter='\\s+')\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the previous session, we clean the data.\n",
    "\n",
    "Here we will go further than last time. We will extract only the `f_*` columns as input features and the `zspec` column as the target. We will keep only positive fluxes, `star_flag = 0`. We could debate on the choice of the features, but we will keep it simple for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [\n",
    "    \"field\",\n",
    "    \"ra\",\n",
    "    \"dec\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"faper_F140W\",\n",
    "    \"eaper_F140W\",\n",
    "    \"faper_F160W\",\n",
    "    \"eaper_F160W\",\n",
    "    \"tot_cor\",\n",
    "    \"kron_radius\",\n",
    "    \"a_image\",\n",
    "    \"b_image\",\n",
    "    \"flux_radius\",\n",
    "    \"fwhm_image\",\n",
    "    \"flags\",\n",
    "    \"f140w_flag\",\n",
    "    \"star_flag\",\n",
    "    \"use_phot\",\n",
    "    \"near_star\",\n",
    "    \"nexp_f125w\",\n",
    "    \"nexp_f140w\",\n",
    "    \"nexp_f160w\",\n",
    "    \"lmass\",\n",
    "]\n",
    "\n",
    "flux_cols = [name for name in df.columns if name.startswith(\"f_F\")]\n",
    "positive_flux = \" & \".join([f\"({col:s} > 0)\" for col in flux_cols])\n",
    "select_zphot = \"(star_flag == 0) & (use_phot == 1) & (z_peak > 0) & (Av >= 0)\"\n",
    "select_zspec = \"(star_flag == 0) & (z_spec > 0) & (Av >= 0)\"\n",
    "\n",
    "select = df.eval(select_zspec + '&' + positive_flux)\n",
    "df = df[select].reset_index()\n",
    "target = 'z_spec'\n",
    "# features = [col for col in df_ if col not in remove + ['z_spec', 'z_peak'] ]\n",
    "features = flux_cols  # + ['Av', 'near_star']\n",
    "\n",
    "\n",
    "df_x = df[features]\n",
    "df_y = df[target]\n",
    "\n",
    "Markdown(f\"\"\"\n",
    "The final data set\n",
    "* contains **{len(df_x):,d} samples**\n",
    "* **{len(df_x.columns):,d} features**\n",
    "* **features** {features}\n",
    "* **target**: {target} \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the \"spectra\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_cols = [name for name in df.columns if name.startswith('f_F')]\n",
    "positive_flux = ' & '.join([f'({col:s} > 0)' for col in flux_cols])\n",
    "x_ = np.array([606, 814, 1250, 1400, 1600])\n",
    "\n",
    "select_zphot = '(star_flag == 0) & (use_phot == 1) & (z_peak > 0)'\n",
    "select_zspec = '(star_flag == 0) & (z_spec > 0)'\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "norm = plt.Normalize(vmin=0, vmax=2)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "select = df.eval(select_zphot + '&' + positive_flux)\n",
    "df_ = df[select]\n",
    "ind = np.random.choice(len(df_), 2_000, replace=False)\n",
    "X = df_[flux_cols].to_numpy()[ind]\n",
    "y = df_['z_peak'].to_numpy()[ind]\n",
    "\n",
    "for xk, yk in zip(X, y):\n",
    "    color = cmap(norm(yk))\n",
    "    plt.plot(x_, x_ * xk, color=color, rasterized=True, alpha=0.4, lw=0.5)\n",
    "\n",
    "plt.yscale('log')\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "cbar.set_label('z phot')\n",
    "cbar.ax.set_visible(False)\n",
    "plt.ylabel('$\\lambda  \\cdot $ flux')\n",
    "plt.xlabel('$\\lambda$ [nm]')\n",
    "plt.title('z phot')\n",
    "\n",
    "\n",
    "plt.subplot(122,  sharex=ax, sharey=ax)\n",
    "select = df.eval(select_zspec + '&' + positive_flux)\n",
    "df_ = df[select]\n",
    "ind = np.random.choice(len(df_), 2_000, replace=False)\n",
    "X = df_[flux_cols].to_numpy()[ind]\n",
    "y = df_['z_spec'].to_numpy()[ind]\n",
    "for xk, yk in zip(X, y):\n",
    "    color = cmap(norm(yk))\n",
    "    plt.plot(x_, x_ * xk, color=color, rasterized=True, alpha=0.4, lw=0.5)\n",
    "plt.yscale('log')\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "cbar.set_label('z')\n",
    "plt.title('z spec')\n",
    "\n",
    "plt.xlabel('$\\lambda$ [nm]')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also convert the fluxes to log10(fluxes) to have a more normal distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.concat([df_[[fk for fk in features if fk not in flux_cols]],\n",
    "                  np.log10(df_[flux_cols]).rename(columns={k: k.replace('f_', 'm_') for k in flux_cols})],\n",
    "                  axis=1)\n",
    "df_y = df_[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [k for k in df_x.columns]\n",
    "\n",
    "wrap = 6\n",
    "\n",
    "nlines = (len(columns) + 1) // wrap +  int((len(columns) + 1) % wrap > 0)\n",
    "\n",
    "plt.figure(figsize=(15, nlines * 4))\n",
    "for e, colname in enumerate(columns, 1):\n",
    "    plt.subplot(nlines, wrap, e)\n",
    "    plt.hist(df_x[colname], bins=43, log=True)\n",
    "    plt.xlabel(colname)\n",
    "\n",
    "plt.subplot(nlines, wrap, e + 1)\n",
    "plt.hist(df_y, bins=43, log=True, color='C1')\n",
    "plt.xlabel(target)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the data into train, test and validation sets\n",
    "\n",
    "Divide the data into train, validation and test sets. We will use the following definitions:\n",
    "\n",
    "* **Training**: The data used to update model parameters (e.g., coefficients or matrix element values).\n",
    "\n",
    "* **Validation**: The data used to update model selection (for instance, we might change hyperparameters of a model based on the validation metrics).\n",
    "\n",
    "* **Testing**: The data used to make final predictions, and possibly evaluate the final model score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è define the training, validation sets. (30% for validation, 70% for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition: single layer regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è define a DNNRegressor model with mutliple layers and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNRegressor(torch.nn.Module):\n",
    "    def __init__(self, n_input: int,\n",
    "                 n_hidden: Sequence[int],\n",
    "                 n_output: int):\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è train the model, plot the learning curves. Be critical of the results. Maybe you need to tune the architecture, the optimizer, the learning rate, etc. Feel free to discuss with the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è Make some plots to understand what your model does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will use the SHAP values to understand the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "torch.set_grad_enabled(True)\n",
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.DeepExplainer(model, inputs)\n",
    "shap_values = explainer.shap_values(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values.squeeze(),\n",
    "                  inputs.numpy(),\n",
    "                  np.array(features),\n",
    "                  plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values.squeeze(),\n",
    "                  inputs.numpy(),\n",
    "                  np.array(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not as good as the tree based methods we explored last week. \n",
    "What can we do to improve the model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
